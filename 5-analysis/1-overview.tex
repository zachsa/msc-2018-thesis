\label{chapter-analysis}
CouchDB's MapReduce implementation is limiting in terms of performing \textit{joins} and \textit{selections} across multiple entities for a number of reasons:

\begin{itemize}
    \item Every document is processed during index calculation; limiting the size of the database footprint greatly improves performance as a result. This can be done by only including entities that are required for index output in a database.
    \item Indexes are derived from a single database (you cannot cross reference multiple databases from a map or reduce function)
    \item Although basic filtering can be performed within a Map function, filters cannot be dynamic and have to be hard coded. For example, it is not possible to write a map function that filters out documents on a field for values found in other documents/indexes in the database. The context in which map and reduce functions are executed is pure JavaScript, with no means of IO to either a file system or over network requests made available \cite{slack28Feb}. Execution of Map and Reduce functions is necessarily idempotent and for this reason, and for security reasons, database state cannot be altered from a map / reduce function during index calculation
\end{itemize}

With CouchDB's means of data retrieval seemingly crippled compared to SQL, it is necessary to keep in mind CouchDB's benefits (as discussed previously) compared to such databases to stay in good humor! In conjunction with external tools such as the nETL software, however, \textit{joins} and \textit{selections} can be implemented with relative ease.

\section{Joining Data}
The three datasets used in this project require joining on the StudentID field. This field appears once for every student in the benchmarks data, several times for each student in the grades data and up to thousands of times per student in the events data. Both the grade and event data contain a field for year - i.e. the year in which a grade was obtained or the year in which an event was registered. As such these two datasets require joining on both the StudentID and Year fields. Benchmark data is only collected once per student enrollment, and as such is associated with grade and event data only by StudentID. To describe the join in terms of SQL, an \textit{INNER JOIN} is performed on the grades/events data, and the resultant dataset is joined to benchmark data via a \textit{LEFT OUTER JOIN}.

Initially an attempt was made at joining the three entities directly via MapReduce; the map function in this case was configured to output identical keys across the three entities for rows that require joining, and tuple as a value in which index position indicates the value-output of specific entities. With the map function output a tuple of 11 indexes instantiated with null values (actually the value `0' to represent falsy numerical values), each index (or group of indices) is reserved for a specific entity type's output and adjusted appropriately during map function execution depending on the type of document the map function is applied to (the map function executes once for every document in the database):

\begin{minted}[linenos=false]{text}
[
   0,                          # i = 0: a course % grade or 0
   0, 0, 0, 0, 0, 0, 0, 0,     # 0 < i < 8: benchmark grade %s
   0, 0                        # 8 < i > 11: event count for semester 1/2
]
\end{minted}

Depending on the value of the `type\_' attribute of the document being processed by the map function, values at relevant indices of the tuple for that particular document type are altered to represent the info in the document that should be output. If the document being processed by the map function is of \textit{type\_} ‘grade’, then the map function emits a tuple with a value at \mintinline{text}{i = 0} and 0s for all other indices: \[[\%, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\]

If the document is of \textit{type\_} `benchmark', the map function emits a tuple with values at \mintinline{text}{0 < i < 8}: \[[0, \%, \%, \%, \%, \%, \%, \%, \%, 0, 0]\]

If the document is of \textit{type\_} ‘event’, the map function emits a tuple with values at \mintinline{text}{8 < i > 11}: \[[0, 0, 0, 0, 0, 0, 0, 0, s1EventCount, s2EventCount]\]

Structuring tuple output, in terms of reserving indices for specific data representations allows for preserving separation of information by document \textit{type\_} even when tuples are processed without context provided by the \textit{type\_}, \textit{StudentID}, \textit{CourseCode} or \textit{Year} attributes - these are removed by the map function; tuples are grouped by common key as output by the map function and passed to the reduce function as a list of tuples per key. The join is performed during execution of the reduce function (in addition to aggregating event document of a single student in a single year into a single document).

The reduce function is passed key:value pairs, where unique keys are associated with a list of tuples; together, these tuples represent information of all three entities, and since they are all passed to a single reduce function a join is achievable (the reduce function is effectively able to process information from all three entities for a single key). The key in this case is compound, and has the form:

\[[StudentId,CourseCode,Year]\]

But the Events data doesn't include Course, and the the Benchmarks data doesn't include Course or Course Year. As such, to allow for Grade data to be joined to Benchmark data on Course and Course Year in addition to Student ID, each Benchmark document needs to be output for every possible combination of Course and Course Year per student. That is the same with the Events documents - each Event needs to be output for every possible course that a student took in a given year.

In terms of performance this approach is disastrous. To analyze 40 courses taken over 3 years, each Benchmark document needs to be emitted for a student $40 x 3 = 120$ times so that the key of the Grade document [Student ID, Course, Year] can always be joined to Benchmark document. Likewise, Each Event data (which has year but not course information) needs to be emitted 40 times - once for each course a join could potentially be performed on. Considering the large number of event documents, this is impracticable. For this approach to be efficient, event documents would need to be aggregated prior to joining to minimize the number of times event data is replicated.

Instead, CouchDB's usage of B+trees as a means of sorting view indexes by keys is utilized to allow for joining the 3 documents in the final dataset. Figure \ref{fig-mapreduce-key-output} shows key output for all three entities in the form: [ID, Course, Year]. Documents that don't have properties for these fields emit 0 instead; resulting in a predictable key format of for each entity that is ordered:

\begin{itemize}
    \item \textit{benchmarks} document: [Id, 0, 0]
    \item \textit{events} document: [Id, 0, Year]
    \item \textit{grades} document: [ID, Course, Year]
    \item \textit{grades} document: [ID, Course, Year]
    \item etc...
\end{itemize}

Although many \textit{events} documents are outputted by the map function per student, due to reduction grouping all these documents by common key, only a single document that is an aggregation of all \textit{events} documents will be stored as reduced output in the view. So, for any student ID, scanning the index iteratively produces first a Benchmark document, then a single (aggregated) Event document, then a single Grade document for each course that student enrolled in. Because output is systematically processed a student at a time, with grades, events and benchmark data iteratively process in a predictable order per student, joins can be achieved by holding documents for a particular StudentID in memory and processing grades, events, benchmarks for that StudentID. When a new StudentID is encountered, a joined row is output, memory is flushed and a new joined row for the new studentID is started. This results in a much more efficient way of aggregating/joining different types of documents for a given set of keys.

\input{5-analysis/figures/fig-mapreduce-key-output}

\section{Selecting Data}
Performing joins via iterating over ordered indices is quite efficient in terms of memory usage. No matter the size of datasets being processed, memory usage will always be fairly low. An increase in size of datasets will simply result in more processing time. Selections performed during the MapReduce stage are therefore completely feasible regardless of the size of the dataset being queried. However, in terms of time, selections are expensive when performed as part of the MapReduce process; the events data as over 40 million records of which a very small number are selected for this analysis. To perform a selection on the events data during MapReduce requires iterating through through all events records - which is expensive in terms of time.

Additionally, it is impossible to select records by a field in which values are specified dynamically. This means that selecting data during MapReduce requires an additional step in data-analysis in which a list of values to use for filtering are determined either via using an additional CouchDB view, Microsoft Excel, a SQL database, etc. CouchDB's MapReduce engine doesn't allow any database, file, etc. interactions by design, and although it's possible to swap out the default MapReduce engine with one that does allow this, this is not recommended. An experiment to implement the couchjs engine using Google's V8 engine (i.e. using node.js) is available on online \cite{v8couchjs}, which is misleading since that is the JavaScript implementation that \textit{node.js} uses. Replacing the default couchjs engine with this one, it would seem as if network requests from map function execution should be possible (making dynamic filtering possible). However, this experiment was abandoned in 2013 precisely because it is too difficult to disable the numerous APIs that the V8 engine makes available to runtime JavaScript code. The CouchDB specification is that the MapReduce execution environment is required to be isolated in terms of interactions with the database or other instances of MapReduce function execution for security and to maintain integrity of the data layer \cite{slack28Feb}.

Primarily for this reason, selection is implemented during the ETL phase of the analysis in nETL. A dynamic filter is used as part of the transformations pipeline, in which a URI to a CouchDB view is specified. The result of this view is used to filter (i.e. perform selection) on data extracted from the CSVs, with only relevant rows from the CSV added to the database. This approach requires configuring an additional index prior to loading data into the database.

This project requires filtering events/benchmark data by students who attended the CSC1015F course. Dynamic selection (i.e. selection via fields using a \textit{JOIN} statement) is implemented via the following workflow:

\begin{itemize}
    \item A CouchDB database of grade documents is created with an index of student IDs for which a grade is available for the CSC1015F course
    \item During ETL, the index of the grades database is retrieved, and rows of events/benchmarks data are filtered according to student numbers as retrieved in the index
    \item Remaining rows after filtering are then loaded into CouchDB where they are indexed, and joined on retrieval
\end{itemize}

\section{Analysis Workflow}
Although the MapReduce function fills the important role of aggregating entities prior to joining them (as discussed in 3-way join section), it is technically possible to join documents directly on retrieval from the main database file since both the main database file and derived indexes are structured as B+trees and can be sorted. However there are three problems with retrieving data directly from the main database file and skipping index creation:

\begin{itemize}
    \item CouchDB database files are sorted by the ``\_id'' field, which when unspecified on document insert is initialized as a UUID. Using UUIDs as unique document identifiers allow for distributed systems in which cluster nodes can operate independently of each other without the possibility of documents being created in separate nodes with conflicting IDs. Even though not required by this project, such best practices are followed. A B+tree sorted by a UUID is not useful for document retrieval, and as such, views are required of the underlying data store for any kind of index-based querying
    \item List functions are invoked via an HTTP GET request with the requirement of specifying a view within the URI. List functions are convenient for usage in this project as they facilitate ordered, iterative, range-based access (meaning that datum can be accessed sequentially, in isolation and in reliable order)
    \item When aggregations of specific entities are required, retrieving data directly from these indexes is vastly easier than having to aggregate during data retrieval. As aggregation logic becomes more complicated the difficulty of such direct data retrieval increases and the benefit of using indexes increases as a result
\end{itemize}

As such, in terms of retrieving processed datasets useful for analysis, for each analysis the nETL and CouchDB applications are incorporated into an workflow represented by the following steps:

\begin{itemize}
    \item CSVs are parsed, rows filtered (selection) and data loaded into CouchDB by the nETL application
    \item An index is created from the CouchDB database via MapReduce
    \item Data is retrieved directly from the index file using a List function, during which time any required joining is performed
\end{itemize}

In terms of the MapReduce tasks, only built-in reduce functions are used; these functions (\_sum, \_count and \_stats) are implemented within the main Erlang process. This offers a performance boost over custom reduce functions since the IO transfer cost between the Erlang process and the view engine (couchjs.exe by default) is avoided. Working on a Windows machine, the IO cost is apparently exaggerated due to the difference between Unix-based and Windows kernel implementations \cite{slack1Nov}.

During analysis, runtime results of the different components of the system are recorded and are shown in Table \ref{performance-analysis}. The metrics include running time of nETL tasks, a summary of the data processed by nETL, CouchDB indexing times, and database/index storage footprints.