% \bibliography{../../bibliography/msc_citations}

\section{MapReduce}
This should be a general overview of how Map Reduce actually works and why it was worth thinking about.

\subsection{What is MapReduce?}
As discussed by \cite{mining2011}, data-centered applications that make use of a large amounts of data have spurred the development of distributed computing using commodity hardware instead of super-computers. <xxx google big table, amazon hive>. The primary benefits of this new architcure are the lower costs and higher avaialability of commodity hardare compared with spcialized hardware <xxx>. The tradeoff of using lower quality hardware, however, is mechanical failure. And so hand in hand with the development of commodity hardware-based computer clusters is development of a fault-tolerant software stack - what \cite{mining2011} have termed the 'New Software Stack'.

This 'new' softare stack involves (amongst other things) a distributed file system the underlies distributed data-processing via a logical framework for data processing called 'MapReduce' \cite{mining2011}. initially implemented as a means for xxx, Google, Amazon and others found it solved the problem of scaling for availability as mentioned in two keys white-papers release by the private companies from research aimed at expanding their businesses \cite{google2008,amazon2007} xxx: actually read these papers.

Subsequently, MapReduce has become something of a hot topic <xxx> with implementations spanning the entire software stack in height and across the breadth of all languages <xxx>. For example, \cite{google2010} shows how using MapReduce can be used directly as a means of searching large amounts of key value stores - which is effectively what all data searching actually can be reduced to: searching key value stores <xxx>. And for this reason, MapReduce is actually a reasonable choice for a conceptual implementation of the SQL specification <xxx>. Inversely, MapReduce can be used in place of SQL to query data that is relational in it's nature - this is discussed at length in \cite{mining2011}, where the authors show several examples of MapReduce-based algorithms to implement SQLesque data operations (joins/aggregations/etc).

MapReduce is actually quite an old idea <xxx> that first rose to prominence as the framework implemented to rank web pages implemented by Google, graph analysis and other such clean data sources <xxx>. But in fact, MapReduce may be just as useful as a means of 'normalizing' irregular data within a standard relational context (the subject of this project) due to the versatility of the different components within the MapReduce specification.

The most popular (open source) implementation of MapReduce is probably Hadoop (\cite{mining2011}), though many other implementations are available. This project indirectly uses MapReduce as part of an investigation of the CouchDB software in it's suitability for analyzing student data at UCT. As such, a requirement of the project is working with the CouchDB implementation of MapReduce specifically.

From \cite{mining2011}, MapReduce can basically be divided into a 3-stage process. (Actually, the term 'MapReduce' is somewhat misleading, and it should probably have been called 'MapGroupReduce' or even 'MapGroup' instead. But that just doesn't roll off the tongue as well).

\begin{enumerate}
    \item A 'mapping' stage: key:value pairs are produced. This key:value data structure is summation of locally-available data on distributed nodes
    \item A 'grouping' stage: A single node collects key:value pairs and groups the separate key:value structures into key:{value, value, value} datasets
    \item A 'reducing' stage: A single node processes the 'values' per key to produce an output. Since processing is done per-key, reduce tasks may be run in parallel on distributed nodes.
    \item Most MapReduce implementations allow for variations of the above. CouchDB implements a 're-reduce' step; i.e. they don't guarantee that all values are processed on the first reduce iteration. Reduce functions written in CouchDB should take this into account.
\end{enumerate}

TODO: Look up CouchDB's MapReduce in terms of whether it is associative and communicative.



\subsection{Data manipulation with MapReduce}
This section should compare standard SQL operations to how they are implemented in MapReduce, with the conclusion that everything is possible that a standard SQL database does, via MapReduce

- Discuss how the book shows it is possible to implement different families of algorithms using MapReduce
- Name a couple of them, and then contrast MapReduce algorithm analysis in terms of time and space complexity
- communication cost model
- complexity theory

The point of me choosing to use MapReduce is that the Map functions are split across many of nodes. And that this makes it easy to calculate dispersedly. So I should get some citations of why this is more difficult and more expensive than doing the same on a relational database. Perhaps this could be as simple as having to maintain a file system that is dispersed across multiple physical computing devices - requiring special hardware.