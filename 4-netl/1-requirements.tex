\subsection{Overview}
Intrinsic to working with databases and data in general is the concept of ETL (Extraction Transformation Loading); i.e. the transformation of data as it is moved from one source (the extraction phase) to another source (a loading phase). Examples of such transformations are scrubbing, filtering, pivoting, grouping, etc. etc. In a traditional sense, these processes are supported by many large database providers (such as Microsoft's \textit{SSDT} and similar software provided by Oracle's \textit{Data Integrator}, IBM's \textit{InfoSphere}, etc. etc.). \textit{SSDT} (formerly known as \textit{SSIS}), for example, provide a visual interface for drag/drop components that get executed within a workflow. The premise is simple: each component has well defined inputs/outputs allowing users to create any number of work flows so long as basic rules are followed per component. Another type of tool - spreadsheets - can also be used when the amount of data is small enough that such programs can do operations on entire datasets at once. Excel, for example, allows for a maximum of around 1 million rows - well short of the required 43 million rows for the Sakai event data for 2016 alone.

It's likely that the availability of of SSDT/SSIS has influenced the uptake of SQL Server in operations that require dealing with large amount of data. It's fair to say that a barrier to using open source software such as CouchDB is the LACK of such software. Bespoke scripts are currently the only viable way of interfacing with CouchDB in a way that is comparable to SQL Server and SSDT. But with high-level languages such as node.js maturing, and the proliferation of small, focused libraries in these languages that abstract much of the unpleasant and gnarly aspects of bespoke scripting (xxx examples), bespoke data-scripting is nowhere near as difficult as it would be within the Microsoft environment (C\# or VB).

As such, working with CouchDB in this project necessitates the development of a bespoke ETL tool. This tool, called \textit{nETL}, is designed as an approximation of a small subset of features as found in \textit{SSDT}. In line with the requirement of transferring large amounts of CSV data from a CSV source to CouchDB, and taking into account the comparatively low entry barrier to bespoke data-transformation scripts, a component of this MSc is an exploration of a possible alternative to SSDT for an environment other than Microsoft's SQL Server. This MSc project actually has several requirements that fall within the ETL spectrum that such a framework could easily be adapted to handle in a generic way. The framework and many plug-in modules is developed under the MIT license and is available at \url{https://github.com/zachsa}. The main requirements of \textit{nETL} for the purposes of this project are listed here:

\begin{enumerate}
    \item Extract data from a source specified by a user into memory
    \item Apply any number of transformations to the data in memory
    \item Examples of required configurations are: converting lines from a flatfile to JSON objects, filtering, whitelisting, creating attributes and values
    \item Load the data from memory into a destination specified by a user
    \item Be generic enough to be applied to multiple different data-processing scenarios such as is required in this project
\end{enumerate}
