\section{Implementation}
The ETL engine and components (extractions, transformations, loads) are implemented in JavaScript (\textit{ECMAScript 2017\textsuperscript{Â®}}) \cite{ecmascript2017} and designed to run in the context of the \textit{node.js} runtime environment \cite{nodejs}. \textit{node.js} emphasizes asynchronous IO, making it a good fit for handling ETL tasks in which IO (CouchDB is accessed exclusively via network requests) accounts for the greatest amount of computational overhead. Since \textit{node.js} runs server-side it provides access via JavaScript to the file system, which is required in terms of an ETL tool.

All components adhere to the modular contract as shown in \ref{fig-module-contract} such that invoking a module returns a \textit{Promise} \cite{jsPromises} that resolves an \textit{invoke} function. This function accepts a single parameter and returns a \textit{Promise} that resolves the result of the module (which differs depending on whether a module performs extraction, transformation or loading operations). JavaScript \textit{Promise} objects are state-representation of asynchronous operations in terms of success and failure of these operations. Since logic implemented in a module may be synchronous, all logic is wrapped within a \mintinline{text}{setImmediate} function to ensure asynchronous execution of all modules.

\input{4-etl/figures/fig-module-contract}

In terms of the application, on startup a \texttt{taskManager} singleton object is created and held in memory indefinitely. A CLI provides an interface by which a user can interact with \mintinline{text}{taskManager} during the program's runtime. Such interactions include starting tasks, pausing tasks, task progress, update status, etc. Once a user directs the application to run a task by the appropriate CLI command with the path to a task configuration specified as an argument, the taskManager runs a task from beginning to end.

On task execution (as directed by a user via the CLI). a task is run from beginning to end iteratively extracting batches of data from a source, transforming that data, loading that data and then repeating the process. Since IO in JavaScript is asynchronous, batching either needs to be run sequentially (batches are processed one after the other), or by carefully managing asynchronous execution of batches. Batches extracted asynchronously and concurrently would quickly overwhelm the network capabilities of any computer since thousands of network requests would be queued instantaneously (network IO is many times slower than file IO, which is many times slower than data transformation). Most of these requests would fail - it is easier to serialize processing of batches than to queue network requests. As such, nETL is implemented to execute tasks concurrently and asynchronously (although asynchronous execution in JavaScript is not truly parallel), but by handling batches of data for a single task sequentially:; a task is processed in terms of a loop in which steps of extraction, transformation and loading are iteratively performed (although handling of batches may be asynchronous during each of these steps).

Components are loaded into the nETL engine on demand (i.e. when a task specified that a competence is required). Modules are loaded once per task; the contract for E, T, and L operations is that Module invocation returns an object with two properties - ``name'': the identifier as used by the application engine to invoke the correct E, T, or L operations during task execution, and the property ``exe'': a pointer to the function that the application engine actually invokes once per task execution. Invoking the ``exe'' returns a function scoped to the current task that holds a reference to the current tasks configuration. This function performs the required component logic and can be invoked many, many times. An example of code defining a module and loading that module into the application is included in the appendix (see \ref{netl-module-loading}). A list of modules as written for, and used in this project is included in the appendix (see \ref{netl-modules}).

\subsection{Extracting data}
Files are read in 64KB chunks from beginning to end within the context of an iterator created by a JavaScript \textit{generator} function \cite{mozillaGenerators}. Chunks are held in memory, split into lines (identified by \textit{LF}, \textit{CR} or \textit{CRLF} line ending markers to allow for cross-platform portability) and yielded a single line at a time to a controlling function executed within the context of the iterative ETL engine. This function iteratively collects \mintinline{text}{n} lines at a time into a list (\mintinline{text}{n} is a user configurable property \textit{batchSize}) and yields ``lists of lines'' - (\textit{batches}). Generators are useful in the regard because they automatically create a state handling mechanism for iterating over file contents - i.e. pointers to positions in files, references to incomplete lines as retrieved from files, etc. Disk access via generators is achieved via code taken directly from an open-source library \cite{bower16}.

Lines in batches are then transformed in parallel, with transformations (specified in configuration) applied to each line in the batch in the order specified in the configuration. Once all transformation have been applied to all lines in the batch (lines can also be discarded from the batch depending on the transformations applied), \textit{taskManager} passes the transformed batch to the loading function (also specified via configuration). This function's contract is such that \textit{taskManager} is notified of when the batch has been loaded successfully to the destination, at which a further batch of lines is generated and processed. etc.

\subsection{Transforming data}
In terms of processing lines in a flatfile (CSV format), headers are only ever read once with the assumption that the all rows can be split into tuples (by some defined delimiter) and that order of the row-tuples corresponds with the order of the columns - if this is not the case, then the CSV is malformed. Transforming rows into objects of key:value pairs is done by creating a reference to the CSV header row that is maintained for the duration of the transformation (headers are specified explicitly in configuration as strings that are split into tuples during runtime). CSV rows are iteratively loaded into memory and split into tuples. Values from row tuples are matched with values from the header tuples by indices to form key:value pairs and create JavaScript objects with similar data representation to as defined by the JSON RFC \cite{rfc7159} (JavaScript objects can easily be serialized to JSON strings). An example of how a row from each data type (data entities used in this thesis) can be represented in object notation is shown in Figures \ref{fig-benchmarks-json-sample}, \ref{fig-grades-json-sample}, and \ref{fig-events-json-sample}.

\input{4-etl/figures/fig-benchmarks-json-sample}
\input{4-etl/figures/fig-grades-json-sample}
\input{4-etl/figures/fig-events-json-sample}

After transforming lines (strings) into lines (objects), additional transformations are applied as a means of controlling what data is loaded into Couch in terms of representation of that data, and what data is included. This is necessary so as to provide a means of identifying CouchDB documents in terms of entity type, reducing the complexity of writing MapReduce indexing functions, and also because reducing CouchDB's footprint allows for performance gains by reducing costs associated with index computation. Since index computation requires that every document in a database be processed, it is helpful (and necessary in the case of large databases) to reduce the number of documents in a database to only the ones that are required. The following transformations are used:

\begin{itemize}
    \item \textbf{Whitelisting entire objects:} Entire objects can be whitelisted based on properties and allowed values for those properties. In addition, objects can be filtered via a list of properties and allowable values for those properties retrieved dynamically from existing CouchDB views
    \item \textbf{Whitelisting object-attributes:} Unneeded attributes can be removed from objects prior to loading into database/other destinations
    \item \textbf{Creating object-attributes:} Additional attributes can be added to objects - i.e. a \textit{type\_} attribute can be added, along with a value as specified by configuration
\end{itemize}

\subsection{Loading data}
Batches of objects are loaded into a CouchDB database via the the HTTP POST \_bulk\_docs endpoint as opposed to separate network requests to CouchDB for each batch-item. Bulk inserts are configured to be atomic - i.e. either an entire insert succeeds or fails. The Load Module as used to load data into CouchDB for this project is shown in the appendix (see \ref{netl-load-couchdb}). Network requests make use of the well-known, open-source node.js library ``request'' \cite{request-lib}.