\section{Implementation}
The ETL engine and components (extractions, transformations, loads) are implemented in JavaScript (\textit{ECMAScript 2017\textsuperscript{®}}) \cite{ecmascript2017} and designed to run in the context of the \textit{node.js} runtime environment \cite{nodejs}. \textit{node.js} emphasizes asynchronous IO, making it a good fit for handling ETL tasks in which IO (CouchDB is accessed exclusively via network requests) accounts for the greatest amount of computational overhead. Since \textit{node.js} runs server-side it provides access via JavaScript to the file system, which is required in terms of an ETL tool. JavaScript is a sensible language in which to implement nETL in the context of this project:

\begin{itemize}
    \item It has a very succinct API making it fast to write code in (i.e. it is a highly abstracted language similarly to Ruby or Python)
    \item But unlike Ruby or Python (and other high level languages), it is opinionated in that it handles IO asynchronously by default
    \item The JavaScript implementation of object-orientation is appealing (to some developers at least \cite{jsBook})
\end{itemize}

All components adhere to the modular contract as shown in \ref{fig-module-contract} such that invoking a module returns a \textit{Promise} \cite{jsPromises} that resolves an \textit{invoke} function. This function accepts a single parameter and returns a \textit{Promise} that resolves the result of the module (which differs depending on whether a module performs extraction, transformation or loading operations). JavaScript \textit{Promise} objects are state-representation of asynchronous operations in terms of success and failure of these operations. Since logic implemented in a module may be synchronous, all logic is wrapped within a \mintinline{text}{setImmediate} function to ensure asynchronous execution of all modules (except for where generators are used, since (\textit{ECMAScript 2017\textsuperscript{®}}) does not support asynchronous generator functions).

\input{4-etl/figures/fig-module-contract}

Components are loaded into the nETL engine on demand and instantiated once per task. The contract for components is that invocation returns an object with two properties - \textit{name}: a means to identify components by the runtime engine, and \textit{exe}: a pointer to the function that the application engine actually invokes once per task execution. Invoking this function returns another function scoped to the current task (called \textit{invoke}), which when invoked performs the required component logic - this function is invoked many, many times. The JavaScript language lacks namespacing and classes as provided by other languages such as Java, C\#, Python, Ruby, etc. etc. \footnote{JavaScript classes are syntactic sugar implemented partially via closure}. As such, a prominent feature in JavaScript code is the extensive use of closure to create scope. A visual representation of such scoping techniques makes them easier to visualize and is shown in Figure \ref{fig-scope}

\input{4-etl/figures/fig-scope}

On task execution (as directed by a user via the CLI) a task is run from beginning to end, iteratively extracting batches of data from a source, transforming that data, loading that data and then repeating the process. Since IO in JavaScript is asynchronous, batching either needs to be run sequentially (batches are processed one after the other), or by carefully managing asynchronous execution of batches. Batches extracted asynchronously and concurrently would quickly overwhelm the network capabilities of any computer since thousands of network requests would be queued instantaneously (network IO is many times slower than file IO, which is many times slower than data transformation). Most of these requests would fail - it is easier to serialize processing of batches than to queue network requests. As such, nETL is implemented to execute separate tasks concurrently; but a single task comprises a series of sequential steps.

JavaScript is not truly parallel - concurrent execution is achieved via adding procedures to an event loop that is executed on a first-in first-out basis via a single thread, with certain operations specified to be implemented asynchronously. Certain functions in JavaScript (\mintinline{text}{setTimout}, \mintinline{text}{setImmediate}, and \mintinline{text}{setInterval}) along with certain environment-provided APIs (such as the node.js filesystem API) pass control back to the event loop before execution of these functions is completed. Such operations allow for specifying a callback function that is added to the event queue at time later from when the asyncrhonous function is first called. So if two tasks are running concurrently and a blocking procedure is called that is not asynchronous, processing of the event queue would be paused until the blocking procedure is completed. As a result, both tasks would be blocked until the procedure completes and the next item on the event loop is processed.

For this reason each step in the ETL engine (extraction, transformation and loading) is implemented asynchronously. Prior to the ECMAScript 2017 specification, an ETL engine implemented in node.js would have been challenging and require complex state management of asynchronous operations. But via the \textit{async}/\textit{await} API (a wrapper for JavaScript \textit{Promises}), such state management is straightforward as shown in Figure \ref{fig-engine} \footnote{Actually the extraction operation (\mintinline{text}{batch = batch.next()}) is NOT awaited (the \mintinline{text}{next} function is not asynchronous) because asynchronous generators are not supported in ECMAScript 2017}.

\input{4-etl/figures/fig-engine}

\subsection{Extracting data}
Files are read in 64KB chunks from beginning to end within the context of an iterator created by a JavaScript \textit{generator} function \cite{mozillaGenerators}. Chunks are held in memory, split into lines (identified by \textit{LF}, \textit{CR} or \textit{CRLF} line ending markers to allow for cross-platform portability) and yielded a single line at a time to a controlling function executed within the context of the iterative ETL engine. This function iteratively collects \mintinline{text}{n} lines at a time into a list (\mintinline{text}{n} is a user configurable property \textit{batchSize}) and yields ``lists of lines'' - (\textit{batches}). Generators are useful in the regard because they automatically create a state handling mechanism for iterating over file contents - i.e. pointers to positions in files, references to incomplete lines as retrieved from files, etc. Disk access via generators is achieved via code taken directly from an open-source library \cite{bower16}.

Lines in batches are then transformed in parallel, with transformations (specified in configuration) applied to each line in the batch in the order specified in the configuration. Once all transformation have been applied to all lines in the batch (lines can also be discarded from the batch depending on the transformations applied), \textit{taskManager} passes the transformed batch to the loading function (also specified via configuration). This function's contract is such that \textit{taskManager} is notified of when the batch has been loaded successfully to the destination, at which a further batch of lines is generated and processed. etc.

\subsection{Transforming data}
In terms of processing lines in a flatfile (CSV format), headers are only ever read once with the assumption that the all rows can be split into tuples (by some defined delimiter) and that order of the row-tuples corresponds with the order of the columns - if this is not the case, then the CSV is malformed. Transforming rows into objects of key:value pairs is done by creating a reference to the CSV header row that is maintained for the duration of the transformation (headers are specified explicitly in configuration as strings that are split into tuples during runtime). CSV rows are iteratively loaded into memory and split into tuples. Values from row tuples are matched with values from the header tuples by indices to form key:value pairs and create JavaScript objects with similar data representation to as defined by the JSON RFC \cite{rfc7159} (JavaScript objects can easily be serialized to JSON strings).

After transforming lines (strings) into lines (objects), additional transformations are applied as a means of controlling what data is loaded into Couch in terms of representation of that data, and what data is included. This is necessary so as to provide a means of identifying CouchDB documents in terms of entity type, reducing the complexity of writing MapReduce indexing functions, and also because reducing CouchDB's footprint allows for performance gains by reducing costs associated with index computation. Since index computation requires that every document in a database be processed, it is helpful (and necessary in the case of large databases) to reduce the number of documents in a database to only the ones that are required. The following transformations are used:

\begin{itemize}
    \item \textbf{Selection-filter:} Entire objects can be whitelisted based on properties and allowed values for those properties.
    \item \textbf{Join-selection-filter:} A list of attributes and values can be retrieved from a \nth{3} party data source (for example from a CouchDB index), and entire objects can be whitelisted based on the retrieved attributes and allowable values for those attributes. This is similar in concept to a join, although no means of actually joining documents is provided (i.e. creating attributes based on data retrieved from a \nth{3} party data source instead of applying selection - although this would be a fairly easy feature to implement).
    \item \textbf{Projection-filter:} Unneeded attributes can be removed from objects prior to loading into database/other destinations
    \item \textbf{Projection-append-attributes:} Additional attributes can be added to objects - i.e. a \textit{type\_} attribute can be added, along with a value as specified by configuration
\end{itemize}

\subsection{Loading data}
Batches of objects are loaded into a CouchDB database via the the HTTP POST \_bulk\_docs endpoint as opposed to separate network requests to CouchDB for each batch-item. Bulk inserts are configured to be atomic - i.e. either an entire insert succeeds or fails. Network requests make use of the well-known, open-source node.js library ``request'' \cite{request-lib}.