ETL in the context of this project refers to loading information from CSV files into memory, transforming representation of the information from CSV format to JSON format, and then inserting that information into CouchDB. Once persisted the data is available for exploration by retrieval from the data store directly, or from customized representation of the underlying data store via CouchDB MapReduce views (indices).

Within the context of RDBMSs, there are a large variety of tools available to assist in ETL processing of CSV source data into databases - for example \textit{Open Studio for Data Integration} \cite{talend}, Microsoft's \textit{SSIS} \cite{ssis} and many, many more options including a host of cloud-based tools such as \textit{zapier} \cite{zapier} that allow for data-exchange between a plethora of different platforms. Zapier, for example, allows inserting data into databases from a Google Sheets spreadsheet. But these tools are not available for CouchDB and so bespoke scripting is performed instead. Initially, scripts developed for this project comprised ETL logic for specific source files and transformations specific to file contents. But this approach quickly became unmaintainable due to the difficulty in making changes to scripts where a lot of code was repeated. As such, a configurable ETL tool was developed and incorporated into the project.

The resultant ETL tool is called \textit{node.js ETL} (nETL) and is designed in terms of \textit{Tasks} that are configured as a pipeline of components. These tasks comprise sequential piping of output from one component to input of another component. Components, by adhering to specified input/output signatures can be strung together in any order allowing for versatile and configurable ETL pipelines.

nETL's core design philosophy is a decoupling of the logic required to manage tasks from the logic of the tasks themselves. Because the logic of running ETL tasks is separate from the more specific logic of how extractions, transformations and loading logic is applied to a data source, components that perform extractions, transformations and loading are defined separately to the main code base and can be loaded into nETL during runtime. In other words nETL is authored as a framework in which custom ETL logic is performed.