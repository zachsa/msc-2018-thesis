Working with data exports requires an ETL (Extraction, Transformation, and Loading) process, in which information stored in tabular form in the CSV files is loaded into computer memory, transformed into JSON-structured objects, and then inserted into CouchDB. Once persisted, data interactions in terms of producing analyses becomes a flexible and agile process with room for creative insight since the cumbersome work of extracting and transforming data into a suitable format for querying does not have to to repeated

Extraction is fairly straightforward; a variable is created in memory to reference the row of the CSV that contains table headers (usually the first row of the CSV). This row is then split by a column delimiter into a tuple of the form [val 1, val 2, val 3, etc]. Then all the remaining rows of the CSV are iteratively loaded into memory in turn, and split into tuples according to a column delimiter. The result of this is iteratively producing a tuple of the same length and form as the header tuple ([val 1, val 2, val 3, etc]) for every row of data. Each row of data can then be manipulated to with each value in the row tuple corresponding to a value in the header tuple at the same index. By grouping the values of the header and row tuple by index, it is easy to convert CSV rows into unordered collections of key:value pairs corresponding to JSON format as specified in the RFC \cite{rfc7159}. An example of how each entity (as discussed previously in \autoref{chapter:data}) can be represented as object structure is shown in Figures \ref{fig-benchmarks-json-sample}, \ref{fig-grades-json-sample}, and \ref{fig-events-json-sample}.

\input{4-etl/figures/fig-benchmarks-json-sample}
\input{4-etl/figures/fig-grades-json-sample}
\input{4-etl/figures/fig-events-json-sample}

Further transformations can then be applied to the entity-objects before being loaded into CouchDB, completing a sequential processing of a single row of CSV data via first extracting information, transforming information, and loading information into the destination (ETL).

In addition to the transformation from tabular rows to objects, specific transformations of the objects are required prior to loading the data to CouchDB. Required transformations used to produce the final results in this project are listed here:

\begin{itemize}
  \item Filtering (both by whitelisting objects and whitelisting object fields) to reduce CouchDBs footprint and reduce indexing computational cost
  \item Adding a `type' attribute to each object for entity classification of objects within CouchDB
\end{itemize}

Loading data into CouchDB involves an HTTP post request with JSON data in the request body. To optimize the ETL process, instead of processing CSVs line-by-line, CSV lines are batched and processed several thousand at a time. As discussed previously, CouchDB is able to handle batched data insertion more performantly than insertion of single documents via the \_bulk\_docs endpoint), and this approach also greatly reduces the number of network requests to CouchDB required to complete loading of all CSV data (which could be expensive in time if ETL was done on a different computer to the CouchDB database software). Batching is also done during CSV file reading to reduce IO overhead since fewer disk reads are required as the percentage of data retrieved per disk read (the batch) increases.

The ETL process is described in terms of coding requirements in pseudo code in Figure \ref{row-object-transformation}, but in terms of implementation, the reality of coding the ETL process is more complicated than is depicted for several reasons as shown here.

\begin{itemize}
  \item Variable (but still valid) formatting that CSVs can have in terms of row delimiters, column delimiters, text qualifiers, character encoding, etc. etc.
  \item Some CSVs don't include a header row, so headers need to be injected into the process during runtime
  \item Real code is substantially longer than psuedo code in general, due to edge cases and implementation of general statements
  \item Generic handling of different CSV sources and CoucDB database destinations require configurable extraction, transformation and loading logic
  \item Generic handling of many different types of transformations that were used during project development but not for the final result (increasing the complexity of the case base):
        \begin{itemize}
          \item Anonymizing object values
          \item Text transformations of values
          \item Transformation from objects back to lines (for CSV output)
          \item Transformation from csv strings to SQL insert strings
        \end{itemize}
  \item During project development, alternatives to CouchDB as load-destinations were required inlcuding for CSV files and SQL Server - as such the code required much adjustment in for these cases.
\end{itemize}

Due to the ever-increasing complexity of the code being used for ETL, the codebase was formalized and structured as a component-based ETL engine that performed ETL tasks based on independent and external configuration objects. These tasks comprise sequential piping of output from one component to input of another component. By providing input/output data contracts, components can be strung together in any order, allowing for versatile and configurable ETL pipelines. By implementing the components as user-configurable, external processes to the ETL engine, specific logic required to process specific information is greatly decoupled from the logic required to extract, transform and load information as a series of high level tasks. The resultant software to achieve this task is implemented in JavaScript (node.js), and is called \textit{nETL} (node.js ETL).

\input{4-etl/figures/fig-netl-logic}