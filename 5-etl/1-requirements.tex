
\section{Requirements}
\textit{nETL} (node.js ETL) was developed as a means of transferring a large amount of data to CouchDB from a flatfile (for example, a CSV); there is no officially released software of this type, so working directly in code is required to achieve this - unlike for RDBMSs where there is a plethora of software available for this task. I am most familiar with SSIS (now known as SSDT). which is largely on which \textit{nETL} is based.

CouchDB stores documents as JSONs, so the \textit{nETL} application takes batches of CSV (or any other type of flatfile) rows, converts each CSV lines into a JSON document and sends batches of JSON documents to CouchDB's \textit{\_bulk\_docs} endpoint. This endpoint allows for sending multiple JSON documents to CouchDB via a single request (by default such inserts are non-atomic, though this is configurable). Batch sizes for \textit{nETL} are configurable; it was found that from a practical point of view both \textit{nETL} and CouchDB were easily able to handle batches of 100 000 lines or more. Smaller batches (\textless 50 000 lines) allow for more frequent user feedback in the form of log output. Taking network speed into account, useful batch sizes are around 10 000 documents in size.

Data manipulation is required to adjust CSV data formats to a CouchDB JSON document, which is also handled by the \textit{nETL} app. Transformations of this sense include filtering rows, whitelisting attribute columns (so as to keep the JSON document size only as large as required), creation of new attributes, etc. In the spirit of other ETL tools such as Microsoft's \textit{SSDT} and similar software provided by Oracle's \textit{Data Integrator}, IBM's \textit{InfoSphere}, etc. etc., \textit{nETL} works via components that can be added to a workflow in any order so long as component contracts (input and output data specifications) are met. \textit{nETL} uses JSON objects for such configuration since the time required for a visual drag-and-drop interface such as provided by Microsoft is unreasonable for this project. Via JSON configuration \textit{nETL} allows the following:

\begin{enumerate}
    \item Extract data from a source specified by a user into memory
    \item Apply any number of transformations to the data in memory (convert tabular data to object data, filtering, whitelisting, attribute creation, basic text transformation (capitalization of value fields))
    \item Load the data from memory into a destination specified by a user
    \item Be generic enough to be applied to multiple different data-processing scenarios such as is required in this project (for example loading from a flatfile to either SQL Server or CouchDB)
\end{enumerate}