\section{Admission-benchmarks Variance}
Admissions data can be used to profile students based on their matric results, NBT scores and a combination of these. In other words a series of benchmarks is created from the admissions data, with the variance of each benchmark assessed. The following matric results and NBT scores are used as benchmarks:

\begin{itemize}
  \item Grade 12 results
        \begin{itemize}
          \item English
          \item Math
          \item Physical Science
        \end{itemize}
  \item NBT (national benchmark test) scores
        \begin{itemize}
          \item AL
          \item QL
          \item Math
        \end{itemize}
\end{itemize}

The data also contains Adv. Math and Math Lit scores, but these are ignored since so few of the students enrolled in CSC1015F took these courses (values of these fields are ``0''). In addition to raw test results in the admissions data, additional derived benchmarks are tested comprising aggregations of the raw scores, including:

\begin{itemize}
  \item Average of Grade 12 results (Eng/Mth/Sci)
  \item Avg of Gr 12 results with double Mth weighting \footnote{UCT previously used this benchmark as a means for evaluating incoming students \cite{sonia2018}}
  \item Avg of Gr 12 results with double Mth/Sci weighting \footnote{UCT currently uses this benchmark as a means for evaluating incoming students \cite{sonia2018}}
  \item Avg of NBT and Gr 12 scores
  \item Avg of NBT and Gr 12 scores with double Gr 12 Math weighting
  \item Avg of NBT and Gr 12 scores with double Gr 12 Math and Science weighting
\end{itemize}

Variance $(\sigma_{\overline{x}})^{2}$ and standard deviation $\sigma_{\overline{x}}$ are worked out for these methods of benchmarking incoming students across admissions data from 2014,2015 and 2016 for students that are residents of South Africa and who attended CSC1015F one or more times during the course of their undergraduate career. Data selection is performed in nETL, data aggregation is performed in the MapReduce functions, and the variance and standard deviation are calculated in the list function during data retrieval.

\subsection{ETL}
Rows are extracted from the admissions data (\mintinline{text}{Admissions (2014 - 2016).csv}) in batches of 5 000. Each row is converted to an object, and rows are selected for undergraduate students that have citizenship or permanent residency in South Africa, and that have a grade for the CSC1015F course. A \textit{type\_} attribute with the value 'admission' is added to each row (now an object). Batches of objects (there are at most 5 000 objects per batch, but usually far fewer due to the filtering) are loaded into a CouchDB database. An example of a row from the admissions data serialized to a JSON string and as loaded into CouchDB is shown in Figure \ref{fig-json-admission}.

\input{5-implementation/figures/fig-json-admission}

\subsection{Index Calculation}
All the admissions documents are loaded into the CouchDB database are mapped to an index consisting of \textit{anonIDnew:[benchmarks]} key-value pairs via a map function. Derivative benchmarks are calculated during map function execution and output to the index as part of the list of benchmarks.

The built-in \_stats function is used to aggregate values in the view-index by benchmark type, which when used without grouping by index key results in MapReduced output of a single tuple containing an object for each benchmark used/created. The resultant index can be retrieved as serialized JSON as shown in Figure \ref{fig-variance-reduce-output}\footnote{Values are rounded for better display} (\mintinline{text}{reduce = true \& group = false}). The \_stats function aggregates benchmark by describing each benchmark in terms of the sum of all students scores for that particular admissions benchmark, the count of how many students are included in the sum, the worst (min) and best (max) scores achieved for each test (or average of tests) by a student, and the sum of squares of each students scores. Because grouping is set to false, reduction is performed (i.e. a \_stats objected is calculated) across the entire dataset instead of for each key output by the map function.

\input{5-implementation/figures/fig-variance-reduce-output}

\subsection{Index Retrieval}
With reference to the reduced (aggregated benchmark) output shown in Figure \ref{fig-variance-reduce-output} variance and standard deviation are worked out according to Equations \ref{eq:variance-with-fields} and \ref{eq:stddev-with-fields} respectively. The computational variation of the sample variance formula is used.
\begin{align}
  (\sigma_{\overline{x}})^{2} =  \frac{sumsqr - \frac{sum^2}{count}}{count - 1}\label{eq:variance-with-fields}
\end{align}
\begin{align}
  \sigma_{\overline{x}} = \sqrt{(\sigma_{\overline{x}})^{2}}\label{eq:stddev-with-fields}
\end{align}
Variance and standard deviation are calculated directly from the reduced index output during data retrieval using a CouchDB list function, with results output in tabular form as shown in Table \ref{tbl-variance-benchmarks}.

\input{5-implementation/tables/tbl-variance-benchmarks}