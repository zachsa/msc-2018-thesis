An understanding of the relationship between indicators such as student grades, online participation, engagement with assigned materials, or demographic information can allow for more adaptive pedagogical approaches to teaching and learning. As such, data mining the massive amount of information that learning management systems (LMS) – such as the University of Cape Town's (UCT) Sakai platform \cite{sakai} – collects has the potential to greatly improve the educational experience. Systems that support data-storage and retrieval are integral to the process of working with these datasets and the logical frameworks that guide the design of such systems are becoming increasingly important as the rate and amount of data collected continues to grow exponentially.

As the implementation and usage of data mining expands, alternatives to traditional relational-orientated databases are becoming the preferred software for housing large data stores. SQL systems such as Oracle, DB2, SQL Server, MySQL and others, tend to scale better vertically than horizontally \cite{couchbaseWhitePaper}. This can be limiting and expensive when compared to NoSQL databases, which allow for the seamless expansion from single to multiple servers.  But swapping out relational-storage for newer alternatives involves a mental shift at many levels within the software stack; this is most evident at the data-retrieval layer, with the shift from using SQL (structured query language) to query data stores to other paradigms that are less familiar to most data professionals (software engineers, software architects, database administrators, etc.). Although many NoSQL databases (databases that don't implement a relational model) implement a version of the SQL standard for querying (which eases the learning curve for new technologies), many do not.

One alternative paradigm to SQL is MapReduce, a logical framework for data querying that allows for the efficient processing of dispersed datasets. This technology, which is already used by several software giants \cite{chandar2010}, is being adopted as part of the new technology stack as part of the ongoing trend of ``information explosion''. As distributed computing power becomes more obtainable through the proliferation of cloud providers such as Digital Ocean, Hetzner, Amazon, Google, etc., it is worth investigating how technologies that make use of dispersed processing via the MapReduce paradigm can do relational operations such as joins, selections, and aggregations.

Also relevant to the shift from relational to non-relational databases is the increasing diversity of the data being collected digitally. As mentioned by the Couchbase project \cite{couchbaseWhitePaper} and observed from general experience in the modern workplace, much of the data produced on a day-to-day basis is semi-structured or unstructured (for example, text documents and spreadsheets). In turn, increasing technological gains, such as those represented by the proliferation of IOT (internet of things) devices, require the housing of ever more varied digital data. RDBMSs seem ungainly in this scenario, with their strictly defined data models making it cumbersome to handle semi-structured and unstructured data. Appropriate systems are expensive in terms of implementation times and complex in terms of architecting and usage. Storing data without having to first define rigid models allows for a more agile approach to data modeling.  Additionally, as a system evolves, subsequent changes to unstructured data models become straightforward and the knock-on effects of code-changes are much more isolated.

\section{Project Significance}
CouchDB is a new technology that embodies much of the NoSQL trend: it has a schema-less data model; it accomplishes data processing via MapReduce; it exists as an open-source code-base, and it is suitable for distribution over commodity hardware.

As an example of CouchDB implementation, this project enables the further development of CouchDB-based applications. With a focus on highly available data and a replication API implemented across multiple types of devices (servers, browsers, tablets, mobile phones, etc.), CouchDB provides a suitable foundation for building offline-first applications that can be used in relatively disconnected locations \cite{ebola2017}. For example, CouchDB was deployed as part of the effort to contain the 2013 - 2016 Ebola outbreak, providing a means of digital data collection in areas with unreliable internet \cite{ebola2017}. Similarly, there is a lot of scope for offline-first application development in the South African context, where data is still very expensive and internet access remains sporadic throughout much of the country.

In terms of this specific case study, insight into joins, selections and aggregating data using CouchDB is discussed in the context of Educational Data Mining (EDM). The effectiveness of UCT's student profiling during the admissions process is discussed in relation to performance in the first-year Computer Science course, CSC1015F, and a means of correlating students’ usage of the Sakai LMS and final course grades is tested.

\section{Motivation \& Aim}
Theoretically, CouchDB is suitable for storing an unlimited amount of unstructured data across distributed clusters of commodity servers \cite{couchdb2.0}. An API that is effectively an interface for the manipulation of b+tree structures is a novel approach to data handling and working with the database directly from an HTTP client is incredibly convenient. Such features make CouchDB a suitable tool for the information-orientated society of the future, where an agile approach to data storage will, by necessity, become the norm as the increasingly interconnected world will produce more and more unstructured data needing to be processed.

In short, CouchDB is an innovative database that allows for innovative systems. Case studies involving CouchDB are therefore necessary to develop an understanding of all the different use-cases that such novel software can represent.