\subsection{Data Overview}
Three different datasets were acquired for this project. Jane Hendry, UCT's CIO, provided data on first-time undergraduates including demographic information, matric results, and admission-acceptance test results as shown in \ref{tbl-fu}. Stephen Marquard, the Learning Technologies Coordinator from the Center for Innovation in Learning and Teaching at UCT (the CILT) provided data from the Sakai platform LMS (Learning Management Software) including course grades achieved by students (\ref{tbl-sakai-grades}) and student interactions with the LMS (\ref{tbl-sakai-events}). The tables show a listing of the fields provided in the CSV export and a description of the field in terms of a description. All three of these datasets include a student ID field, which was anonymized by Associate Professor Sonia Berman and, in the case of the Sakai interaction data, by Stephen Marquard. The anonymized student ID number field is anonymized consistently across all three datasets, making these datasets relatable and suitable for joining.

\textit{Course grade} data was received for 2014, 2015, and 2016. The field \textit{course\_id} was found to not correspond with the \textit{event} data field \textit{site\_key}, meaning that the \textit{event} and \textit{grade} data is only relatable by year and student ID, and not per individual course.

Demographic data (the \textit{FU} dataset) was includes the years 2014, 2015 and 2016. The schema of this dataset is not consistent across the different years, since in addition the registrant data, a row includes a summary of academic performance at UCT for subsequent years. As a result there were many repeated fields in these datasets and required manual processing before they were of use. Rows from all years of the demographic data were combined into a single sheet with a common list of fields as shown in table \ref{tbl-fu}. The grade data from all three years was also combined into a single sheet in Excel (without altering any field names of any entities). Event data was only received for the 2016 year.

Using the \textit{nETL} application, lines from the flat data files (CSV files) were extracted in batches of between 10 000 rows and 50 000 rows at at time and loaded into CouchDB. Data-transformation and cleansing, including translating data from tabular format to JSON format, filtering rows, and white-listing attributes was performed as part of this ETL process.

The difference between data in a tabular format (such as a CSV) compared to JSON documents is that every JSON document (row from a CSV) needs both attribute and value information, whereas in visual tables, attributes are specified once as an ordered list once. Rows then consist of ordered values relatable to the value of the attributes list at the corresponding index. Visually (i.e. when working on an Excel file), such association is trivial. JSON documents comprise a list of unordered \textless key:value \textgreater pairs as stipulated in \cite{rfc7159} necessitating the need for a computerized transformation process. Transforming CSV data into JSON data requires maintaining a list of attribute names, iterating through rows and applying those attribute names to values in those rows. The CREATE\_OBJ\_FIELD \textit{nETL} module was composed for this task, the code of which is available in the appendix (\ref{netl-trans-create-obj-field}). A representation of each entity as a JSON object is shown in \ref{tbl-json-examples}.

A batch of data extracted from the CSV files comprised a list of rows (a row comprised a list of values), each of which were individually transformed into JSON documents in this way and stored as a list of documents. This list was then transformed several times and eventually loaded into CouchDB as a list of JSON documents. The source code for \textit{nETL} module responsible for extracting CSV lines is shown in the appendix at \ref{netl-extract-flatfile}, and the source code for the module used to load the data into CouchDB is shown in the appendix at \ref{net-load-couchdb}. Extracting values from a CSV was trickier than anticipated due to surprising variation in answers to the question "What is a CSV?". Variations in CSV formatting result in complexities with delimiting values; a 3rd party CSV-parsing library was used for this task.

The FU data was filtered on two attributes: \textit{Career} with the values "UGRD", "First Year", "Second Year" and "Third Year" accepted, and \textit{Citizenship Residency} with the values "SA Citizen", "Permanent Resident", "C" and "P" accepted. Sakai course grades were filtered on four attributes: \textit{RegCareer} with the value "UGRD" accepted, \textit{Course} with only specific courses accepted (dependent on the analysis), \textit{CourseCareer} with the value "UGRD" accepted, and \textit{CourseSuffix} with the values "F" (semester 1), "S" (semester 2), "H" (single semester course over whole year), and "W" (whole year course) accepted. Sakai events were filtered on the the attribute \textit{event\_id}, with only the value "281" accepted (corresponding to presence events). Filtering was performed by the \textit{nETL} application by the FILTER module, with source code shown in the appendix \ref{netl-trans-filter}.

Following filtering, a white-listing transformation was applied to reduce the number of attributes/document where those attributes were superfluous. In the case of the FU data, no white-listing was needed since unnecessary columns had already been removed when the flat files were manually aggregated in Excel. Grade data was white-listed extensively since only the \textit{Percent} field was really of interest in this study. White-listing was also performed by the \textit{nETL} application; code for the WHITELIST module is shown in the appendix \ref{netl-trans-whitelist}.

A further transformation was applied to all JSON documents within the \textit{nETL} application; an attribute "type\_" was added to each document to identify documents as conceptually belonging to a specific entity (an FU row, a Sakai Grade row, or a Sakai Event row). This was achieved via the CREATE\_OBJ\_FIELD module shown in the appendix \ref{netl-trans-create-obj-field}.

In addition to data transformations applied suring the ETL process by \textit{nETL}, two fields - \textit{Percent} in the grade data and all the percentage fields in the FU data - had inconsistent data types. These fields need to be analyzed as numerical values (i.e. percentages) but were exported as a mixture of percentages, symbols, and abbreviated statuses. These fields were normalized during the MapReduce process discussed later in this project. A description of the different values that needed to be normalized in the Grade data is shown in \ref{tbl-sakai-grades-percent}, with numerical equivalents applied based on estimates of circumstance. Translation of the FU data involved a straight conversion of symbols to numerical numbers ("A" was translated to "80", "B" was translated to "70", etc.).

Achieved via a combination of filtering and white-listing in \textit{nETL}, and further scrubbing in the MapReduce functions, the resultant datasets used for analysis, after scrubbing, filtering, etc. include: 1) benchmarks of undergraduate students that registered for UCT in 2014,2015,2016. 2) Undergraduate course results in 2014, 2015, 2016 for courses that ran in the first semester, second semester, or during the normal school year. 3) A count of 'presence' events for students in 2016 on the Sakai LMS.