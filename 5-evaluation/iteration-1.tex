I took an iterative approach to joining entities, with each iteration introducing additional complexity to the join requirements and introducing additional data-load. With an iterative approach I was able to identify key performance considerations when working with the tech stack described within this thesis. Looking at two entities: \textit{Grades} and \textit{Demographics} with the aim of correlating student benchmarks with the results of a specific course \textit{CSC1015F}. Student results from the \textit{CSC1015F} were iteratively joined with an increasing number of the available benchmarks to measure the indexing time of creating the joined datasets.

In preparation of this assessment \textit{nETL} was configured to process all the cohort demographic data from 2016,2015,2014 and course grade data from 2016, 2015, 2014. This includes 2 CSVs (available on request) that formed the basis for all iterative analyses. The CSVs are listed here:

\begin{enumerate}
    \item \textit{FU-CombinedGrades-2016Reg.csv}
    \item \textit{CombinedGrades.csv}
\end{enumerate}

\subsection{iteration 1: \textit{CSC1015F} result vs English Gr12 result}
Filtering was applied to the demographic data to load only students who had taken \textit{CSC1015F}. Because \textit{nETL}'s filter module allows for specifying attributes to filter on with a corresponding list of values to accept, and NOT (without additional coding) filter on indirect associations such as \textit{student IDs that appear in the set ...}, a list of students was manually compiled to apply the filter on comprising 1879 unique students who registered for \textit{CSC1015F} in 2016, 2015, and 2015 (36 students registered for this course in separate years). Grades were filtered to only load results from the \textit{CSC1015F} course. White-listing of attributes was then applied to create docs of the following form:

\begin{minted}{javascript}
// Demographics JSON documents
{
    "type_": "demographic",
    "anonIDnew": <number>,
    "Gr12 Eng": <grade (uncleaned)>,
    "RegAcadYear": <year>
}

// Grade JSON documents
{
    "type_": "courseGrade",
    "RegAcadYear": <year>,
    "anonIDnew": <number>,
    "Percent": <grade (uncleaned)>
}
\end{minted}

The \textit{nETL} configuration comprised two tasks run asynchronously to produce the above entities. The \textit{nETL} configuration is shown in \ref{appendix:config-i1}, with execution results of the \textit{nETL} shown in \ref{i1-results} alongside the database document summary (produced by the MapReduce index shown in \ref{appendix:dbInfo}).

\begin{table}[]
    \centering
    \caption{\textit{nETL} runtime results for I1 analysis}
    \label{i1-results}
    \begin{tabular}{lcc}
                                      & FU documents                    & Grade documents \\ \hline
        CSV lines extracted           & 12219                           & 513872          \\
        CSV lines loaded              & 1380                            & 1891            \\
        \textit{nETL} task time (sec) & 2.4                             & 31.9            \\
        CouchDB footprint             & \multicolumn{2}{c}{0.8MB}                         \\
        Year 2016 doc-count           & 524                             & 738             \\
        Year 2015 doc-count           & 460                             & 618             \\
        Year 2014 doc-count           & 396                             & 535             \\
        View calculation time (sec)   & \multicolumn{2}{c}{\textless 5}                   \\
    \end{tabular}
\end{table}

The MapReduce join in the context of this analysis should be performed on key tuples of <year, studentId>, with a result output of <\textit{CSC1015F} \%, Gr12Eng \%>. The design document to achieve this format is shown in \ref{appendix:ddoc-i1}. The Map function includes logic to translate grade symbols to percentages; since the reduce phase (the built-in \_stats function in this case) requires that value-output of the Map function be an array of numbers. Results are shown in the graphs in Fig TODO.

\subsection{iteration 2: \textit{CSC1015F} result vs all (8) demographic markers}
Similar to \textit{Iteration 1}, filtering was applied to the demographic data to load only students who had taken \textit{CSC1015F} by manually selecting student IDs and configuring the filter to allow rows of only those student IDs into the database. Grade-rows from the \textit{CSC1015F} course and white-listing of attributes was then applied to create documents with the desirable attributes only. This iteration used all the markers from the demographic information:

\begin{enumerate}
    \item Gr12 Eng \%
    \item Gr12 Sci \%
    \item Gr12 Mth \%
    \item Gr12 Mth Lit \%
    \item Gr12 Mth Adv \%
    \item NBT AL \%
    \item NBT QL \%
    \item NBT Mth \%
\end{enumerate}

\begin{minted}{javascript}
// Demographics JSON documents
{
    "type_": "demographic",
    "anonIDnew": <number>,
    "Gr12 Eng": <grade (uncleaned)>,
    "Gr12 Sci": <grade (uncleaned)>,
    "Gr12 Mth": <grade (uncleaned)>,
    "Gr12 Mth Lit": <grade (uncleaned)>,
    "Gr12 Mth Adv": <grade (uncleaned)>,
    "NBT AL": <grade (uncleaned)>,
    "NBT QL": <grade (uncleaned)>,
    "NBT Mth": <grade (uncleaned)>,
    "RegAcadYear": <year>
}

// Grade JSON documents
{
    "type_": "courseGrade",
    "RegAcadYear": <year>,
    "anonIDnew": <number>,
    "Percent": <grade (uncleaned)>
}
\end{minted}

Similarly to Iteration 1, \textit{nETL} was configured to run two tasks asynchronously with the configuration shown in \ref{appendix:config-i2}. Execution results are shown in \ref{i2-results}.

\begin{table}[]
    \centering
    \caption{\textit{nETL} runtime results for I2 analysis. As expected the database footprint increased slightly along with inclusion of 7 additional columns from the demographics CSV. Runtime of the \textit{nETL} task increased slightly - but that is not meaningful on a single run.}
    \label{i2-results}
    \begin{tabular}{lcc}
                             & FU documents              & Grade documents \\ \hline
        CSV lines extracted  & 12219                     & 513872          \\
        CSV lines loaded     & 1380                      & 1891            \\
        nETL task time (sec) & 2.4                       & 33.8            \\
        CouchDB footprint    & \multicolumn{2}{c}{0.9MB}                   \\
        Year 2016 doc-count  & 524                       & 738             \\
        Year 2015 doc-count  & 460                       & 618             \\
        Year 2014 doc-count  & 396                       & 535             \\
    \end{tabular}
\end{table}


\subsection{iteration 3: Introducing additional courses}
The above examples achieve the join of the demographic data and the grade data through joining on keys as output by the map task. CoucuDB allows for configuring the level at which keys are grouped when using \textit{compound-keys}. The above iterations showed key groupings in the form \textit{<year, student ID>}. With an \textit{exact} level of grouping (the default), demographic and grade data is joined on the \textit{<year, student ID>} combination and the output is grouped on that as well. With a grouping of level 1 it's possible to join the two entities on just \textit{year}, and then group on that as well.

Such a grouping is only possible when all entities can output keys on which grouping can occur - i.e. both the demographic and grade data include \textit{year} and \textit{student number} fields, and so by emitting \textit{<year, student ID>} tuples from each entity allows the CouchDB MapReduce engine to group on matching tuples. The demographic entity doesn't include course information, however, so it is impossible to (natively) emit tuples of \textit{<year, student ID, course ID>} on which the entities can be grouped.

Using built-in reduce functions (\_sum or \_stats), where the values-output of the map function must be numbers, there are 3 different ways incorporating courses into the join as discussed in iteration 1 and iteration 2. These are:

\begin{enumerate}
    \item
\end{enumerate}


NOTE:
for the demographic, i would need to emit the same document for several keys. this would be something like an outer join