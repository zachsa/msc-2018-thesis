\subsection{Data}
Three different datasets were acquired for this project. Jane Hendry, UCT's CIO, provided data on first-time undergraduates including demographic information, matric results, and admission-acceptance test results (the \textit{FU} dataset). Stephen Marquard, the Learning Technologies Coordinator from the Center for Innovation in Learning and Teaching at UCT (the CILT) provided Sakai \textit{grade} and \textit{event} data. Each of these datasets include a student ID field, which was anonymized by Associate Professor Sonia Berman and Stephen (in the case of the \textit{event} data). The three datasets are relatable due to the anonymized student ID field which is common to all three entities.

\textit{Event} data was received for the 2016 academic year and comprises approximately 43 million rows of Sakai events of different 'event types'. This project considered only \textit{events} of type \textit{presence}, identified via an \textit{event_id} of the value \textit{281}. This translates to roughly 13 million applicable events (i.e. rows). \textit{Course grade} data was received for 2014, 2015, and 2016. The field \textit{course_id} was found to not correspond with the \textit{event} data field: \textit{site_key}, meaning that the \textit{event} and \textit{grade} data is only relatable by year and student ID, and not per individual course. Demographic data was used from 2014, 2015, 2016. Each of the years included different data fields (since 2014 registrants also had 2015 and 2016 results appended). As a result demographic data was pre-processed using Excel to normalize the entities. The grade data was also pre-processed in Excel to provide a single sheet of all three years (easier to load into a database like this); but the column names in this case remained constant. For a description of the three data entities provided by UCT refer to the appendix (\ref{appendix:data}), where a description of each field is shown and the basic filtering that was done on the CSV exports. Also in appendix \ref{appendix:data} is an example of each entity represented as JSON and as used by CouchDB. Filtering as mentioned on tables (\ref{event-data-csv}, \ref{grade-data-csv}, \ref{demographic-data-csv}) was performed in the \textit{nETL} application, specified as a \textit{transformation}. The module is shown in the appendix at \ref{netl-trans-filter}. In addition to filtering, an attribute was added to each row imported from the CSVs: \textit{\_type} - so that each document can be identified as a conceptual member of a particular entity. This additional attribute was appended to each row also via the \textit{nETL} application; an attribute \mintinline{json}{{"type\_": "<entity name>"}} was added to every CSV line extracted. Code to show how such a \textit{transformation} is achieved is included in \ref{netl-trans-create-obj-field}.