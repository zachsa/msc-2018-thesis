\subsection{Setting up CouchDB}
CouchDB 2.0 onwards implements clustering functionality that makes large-scale, sharded databases feasible. For the purposes of this project, clustering was tested on 3 Hetzner servers CX20 servers with indexing tested first locally on a Windows machine and then on cheap CX20 servers. The commands to provision Ubuntu Xenial and CouchDB 2.1 have been produced below. For the purposes of this project, these commands are executed via a Ruby script so as to automate the server provisioning process for any number of (existing) virtual servers. Hetzner, Digital Ocean, Google, AWS and many other cloud providers make an API available that allows for automated provisioning of \textit{n}-virtual servers, where \textit{n} is limited only by one's ability to pay!

Since virtual servers are relatively inexpensive in comparison to dedicated servers, and easier to setup, dispersed data indexing is extremely-achievable using CouchDB. CouchDB MapReduce queries could easily be dispersed among thousands of computer nodes, as could massive amounts of data. In terms of database size limits, it would theoretically be as easy working with several PB of data and 10 000 + CouchBD nodes as it is working with a small cluster.The Ruby script is called 'rCluster', with code available at \url{https://github.com/zachsa/rcluster}.

Since CouchDB 2.0 onwards is designed primarily with clustering in mind, by default databases are created with several (8) shards. A database with multiple shards per a single node is considered to be \textit{oversharded}, which is in itself fine. For the purposes of this project the default of 8 shards (\textit{q=8} in Couch-speak) is used across all servers (since the maximum number of nodes tested on is 6).

\begin{minted}{sh}
# Set hostname of server
hostname <hostname>; rm /etc/hostname; touch /etc/hostname; echo <hostname> >> /etc/hostname; chmod 466 /etc/hostname;

## Install basic tooling 
# GCC collection (GNU make and GNU compiler tools)
apt-get update
apt-get install build-essential -y

# Update openssl to 1.0.2l
cd /usr/src
wget https://www.openssl.org/source/openssl-1.0.2l.tar.gz
tar -zxf openssl-1.0.2l.tar.gz
cd openssl-1.0.2l
./config
make
make test
make install
mv /usr/bin/openssl /root/
ln -s /usr/local/ssl/bin/openssl /usr/bin/openssl

# Python
apt-get update
apt-get install python -y

# libcurl
apt-get update
apt-get install libcurl4-openssl-dev -y

# ICU
apt-get update
apt-get install libicu-dev -y

# Pre-seed debconf to answer CouchDB installation wizard automatically
debconf-set-selections <<< 'couchdb couchdb/bindaddress string 0.0.0.0'
debconf-set-selections <<< 'couchdb couchdb/cookie string monster'
debconf-set-selections <<< 'couchdb couchdb/mode string clustered'
debconf-set-selections <<< 'couchdb couchdb/nodename string couchdb@<hostname>'
debconf-set-selections <<< 'couchdb couchdb/adminpass password <password>'
debconf-set-selections <<< 'couchdb couchdb/adminpass_again password <password>'

# register CouchDB package with the server package manager and install
echo 'deb https://apache.bintray.com/couchdb-deb xenial main' | sudo tee -a /etc/apt/sources.list
curl -L https://couchdb.apache.org/repo/bintray-pubkey.asc | sudo apt-key add -
apt-get update
apt-get install couchdb -y
\end{minted}

And then once those commands have been run to setup all the CouchDB nodes, the CouchDB cluster can be configured using a few commands on any one node (temporarily referred to as the \textit{CoOrdinatingNodeHost}):

\begin{minted}{sh}
# Run these two lines to add a node to the CouchDB cluster
curl -X POST -H \"Content-Type: application/json\" http://<username>:<password>@<CoOrdinatingNodeHost>:<port>/_cluster_setup -d '{\"action\": \"enable_cluster\", \"bind_address\":\"CoOrdinatingNodeHost\", \"username\": \"<username>\", \"password\":\"<password>\", \"port\": <port>, \"node_count\": \"<intented node count>\", \"remote_node\": \"<remote hostname>\", \"remote_current_user\": \"<username>\", \"remote_current_password\": \"<password>\" }'
curl -X POST -H \"Content-Type: application/json\" http://<username>:<password>@<CoOrdinatingNodeHost>:<port>/_cluster_setup -d '{\"action\": \"add_node\", \"host\":\"<remote hostname>\", \"port\": \"<port>\", \"username\": \"<username>\", \"password\":\"<password>\"}'

# Finalize the cluster setup
curl -X POST -H \"Content-Type: application/json\" http://<username>:<password>@<CoOrdinatingNodeHost>:<port>/_cluster_setup -d '{\"action\": \"finish_cluster\"}'
\end{minted}