As mentioned previously, CouchDB views are optimized when using built-in reduce functions, with custom reduce functions performing most poorly on Windows machines. As this project was completed on the Windows OS, the analysis on how best to aggregate the different entities (grades, demographics and events) was confined to using just the \textit{\_stats} built in reduce function, since the output of this function overlaps output of the other two built-in reduce functions (\textit{\_count}, \textit{\_sum} (and provides additional metrics). The \textit{\_stats} function signature requires that map output adhere to the following constraints; that values are either a single number or a list of numbers, that values (if lists) should be of the same length, and that indexes in value-lists each correlate with each other in terms of what the value represents. Using custom reduce functions would greatly increase the number of possible methods of joining entities since output of map functions would not be constrained to match the contracts of the built-in functions. Such an approach shouldn't be discounted considering that on platforms other than Windows, reduce function calculation (whether custom or built-in) represents a small percentage of computer resources used in view calculations overall (see appendix \ref{slack-1-nov}). And in any case, a system that utilizes CouchDB is likely to be based on a cluster of Linux machines rather than a single Windows machine.

Joining across the 3 entities (Demographics, Grades and Events), in adherence with the \textit{\_stats} function contract, requires emitting common keys in the map function on which grouping can be performed. To maintain the resolution of the Grade data, that is; \textit{results of a particular student in a particular year for a particular course}, a compound key of the tuple \textless studentID, courseCode, year \textgreater is used for grouping, i.e. for every document processed by the map function, a key:value pair with a key in the form of that tuple needs to be emitted. Emitting this compound key is straightforward for grade data since each document of \textit{type\_} courseGrade has fields for all three of these values. However, neither the demographic nor the event data contains fields for 'courseCode', and student benchmarks in the demographic data are only from the year that student first registered.

To produce a key of the tuple \textless studentID, courseCode, year \textgreater for demographic data, the courseCode and year value have to be fabricated. In other words, demographic data has to be duplicated for every possible key combination on which it could be joined. In this case a single demographic document needs to be output for every course that a student can take, and then further duplicated for every year in which a student could take that course. Likewise the event data, which has fields for studentID and year, needs to be duplicated for every course that a student can take. Such an approach to joining is the basis of this analysis, which is done in iterations. Each iteration increases the volume of data processed by the map function so as to get insight in the effectiveness of this approach to joining documents. Analyses are discussed in terms of the results of each iteration. In general, each analysis involves 4 phases of data-wrangling:

\begin{enumerate}
    \item Configuring \textit{nETL} to load the required data
    \item Writing a Map function to perform the join (along with the \textit{\_stats} reduce function)
    \item Writing a List function to transform the MapReduce output into tabular data
    \item Manipulation of derived dataset in Excel
\end{enumerate}

\subsection{Join of Grades/Demographics}
For a join on the Grades and Demographics entities, the map function is configured to output key-value pairs of the form: \textless studentID, courseCode, year \textgreater : <9 element list>. A description of the 9-element value list is shown in \ref{grades-join-demographics-output}. Configuration for the \textit{nETL} task is shown in the appendix (see \ref{netl-config-grades-join-demographics}), as is the Map function and list function (see\ref{msc-design-doc}).

\begin{figure}[ht]
    \centering
    \begin{minted}{javascript}
 [
    // List function to exclude where sum is 0
    "CSC1015F %",

    // Exclude where all below are 0 
    "Gr12 Eng %", 
    "Gr12 Sci %",
    "Gr12 Mth %",
    "Gr12 Mth Lit %",
    "Gr12 Mth Adv %",
    "NBT AL %",
    "NBT QL %",
    "NBT Mth %",
 ]    
    \end{minted}
    \caption[Analysis 1: Grades joined with Demographics]{\textbf{Figure \ref{grades-join-demographics-output}: Output of map function for grades joined with demographics.} This list, shown as a JavaScript array, is the key to the map function output. The comments indicate additional treatment as performed in the filtering function.}
    \label{grades-join-demographics-output}
\end{figure}


\subsubsection{CS1015F}

\subsubsection{..., ..., ..., ...}


\subsection{join of Grades/Demographics/Events}







\begin{table}[h]
    \begin{threeparttable}
        \textbf{Table \ref{performance-analysis}}\par\medskip\par\medskip
        \caption[Software performance analysis]{Running time analysis of \textit{nETL} tasks and CouchDB MapReduce indexing}
        \label{performance-analysis}
        \begin{tabularx}{\textwidth}{>{\hsize=0.5\hsize}Y>{\hsize=0.8\hsize}X>{\hsize=0.8\hsize}X>{\hsize=1.9\hsize}X}
            \toprule
            \mC{c}{}                                               & \mC{c}{Grade-Demographic joins} & \mC{c}{} & \mC{c}{} \\
            \midrule
            Demographic lines extracted                            & 12 219                          &          &          \\
            Demographic lines loaded                               & 9 874                           &          &          \\
            Demographic task time (sec)\tnote{\textsuperscript{1}} & 3.517                           &          &          \\
            Grade lines extracted                                  & 513 872                         &          &          \\
            Grade lines loaded                                     & 305 252                         &          &          \\
            Grade task time (sec)\tnote{\textsuperscript{1}}       & 61.421                          &          &          \\
            CouchDB footprint (MB)\tnote{\textsuperscript{2}}      & 79.5                            &          &          \\
            View calculation time (sec)\tnote{\textsuperscript{3}} & 2112.129                        &          &          \\
            View size (GB)                                         & 2.29                            &          &          \\
            \bottomrule
        \end{tabularx}
        \scriptsize
        \begin{tablenotes}
            \item[\textsuperscript{1}]Tasks are run asynchronously, so time taken includes processing of other tasks in this run. Task run times are printed out to the log
            \item[\textsuperscript{2}]This is representative of the amount of data processed by \textit{nETL}
            \item[\textsuperscript{3}]CouchDB views are calculated per shard. By default a database contains 8 shards (even in single node mode). The log file shows start and end times of view calculations for each shard, the time is taken as time the first shard starts indexing, to the time the last shard stops indexing.
        \end{tablenotes}
    \end{threeparttable}
\end{table}


















\begin{figure}[ht]
    \centering
    \begin{verbatim}
+----------+---------+------+----------------+
|   Col1   |  Col2   | Col3 | Numeric Column |
+----------+---------+------+----------------+
| Value 1  | Value 2 | 123  |           10.0 |
| Separate | cols    | x    |       -2,027.1 |
+----------+---------+------+----------------+
    \end{verbatim}
    \caption[Analysis dataset output format]{\textbf{Figure \ref{dataset-output}: Format of output dataset.} This project outlines the steps taken to produce a dataset of this format.}
    \label{dataset-output}
\end{figure}


todo: output CSV,



