\section{Tests}
Accurate nETL application component functionality is confirmed via unit testing, as are map and list functions used for index creation and retrieval. Unit tests are written using the open source JavaScript \textit{Mocha} testing framework \cite{mochaTest}. In addition, a small sample of the data was processed manually by the same procedures as the analyses conducted in this project to confirm that the processes work as expected and that statistical calculations produce results as expected.

\subsection{nETL Unit Tests}
The basic premise of the ETL process is that the lines are extracted from CSVs and loaded into CouchDB reliably. Assertions are used to ensure that each nETL module (the extraction module, the transformation modules, and the loading module) perform as expected. No integration tests are performed except by manually checking that the test data is loaded into CouchDB in the anticipated format - which is indeed the case.

Tests relating to CSV-line extraction assert that all lines from the CSV are extracted iteratively and not all loaded into memory at once, and that all lines are extracted from CSVs. Tests for parsing CSVs into objects ensure that CSVs are treated according to the RFC 4180 specification; qualifiers are handled correctly, columns line up correctly with the headers, values are handled correctly, and lines are correctly transformed into JavaScript objects. In terms of selection, unit tests ensure that objects may be filtered for individual values for up to multiple attributes, that objects can be filtered any number of values on any number of attributes, and that filtering is done on an all-or-nothing-basis (objects either meet all filter requirements or are returned as ``null''). Unit tests assert that creation and whitelisting of attributes works as expected.

\subsection{Manual Map \& List Function Tests}
Manually testing each of the processes described in Chapter \ref{chapter-analysis} was done using small dummy datasets with just 5 IDs. Based on this dataset, the MapReduce output of test data is as expected:

\begin{itemize}
  \item Correlation between Grades and admissions benchmarks
        \begin{itemize}
          \item Document output is ordered key[0], then[1], then key[2] - For every student output of benchmark data is followed by that student's grade data
          \item Multiple results from the same course are output in order, ordered by course registration date.
          \item Value output for the Grade documents is a single number, and output for the Benchmark document is an array of 8 numbers (that correspond to the CSV input)
          \item No documents appear that should be filtered out
          \item Output contains the correct number of documents
        \end{itemize}
  \item Correlation between Sakai presence events and $\Delta ClassRank$
        \begin{itemize}
          \item Event counts are correct, and the output format is correct for semester 1 and semester 2 for each student
          \item Document ordering is correct for each student (benchmarks output, followed by events output, followed by \textit{grades} output)
          \item Grade documents from years other than 2016 are not included in the output
        \end{itemize}
\end{itemize}

List function output of the test data shows that rows are joined correctly for both correlation analysis of grades (A) and event/$\Delta classrank$ (B). One particular case worth testing is that when MapReduce output includes benchmarks and event counts, but no grade documents for a student, the List output does not include that student. This case often occurs when a student's took CSC1015F in a year other than 2016, and that students event data includes usage for courses other than CSC1015F in 2016. Statistical calcluations were checked for accuracy using Microsoft Excel (when possible).