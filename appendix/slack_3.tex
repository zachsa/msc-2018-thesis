\subsection{Slack conversation 3}
\label{appendix:slack3}

zach [12:25 PM]
Question @rnewson - thanks for those pointers yesterday. it took just over an hour to finish indexing the MapReduce view using the \_sum reduce function. In an unrelated note, I remember reading somewhere that the default reduce functions run much more efficiently than custom reduce function? why is this? i also recall someone mentioning (I think it was on this channel) that the reduction phase is done by the main erlang process. Does this mean that reduction is not done in parallel? i.e. the map functions are computed using instances of couchjs, but the reduce function is done via a single process?


zach [12:48 PM]
ah. i see why there is a performance difference here: http://docs.couchdb.org/en/2.1.0/maintenance/performance.html\#builtin-reduce-functions. I'd still be interested to know if the reduce function is done by many 'reducers'


jan [12:55 PM]
per view build, it is sequential, optimising for write IO / on a sharded system, view builds are per-shard


zach [1:10 PM]
is it correct to say that for a particular view: per shard, map indexes are calculated in parallel (multiple couchjs processes), followed by a single instance of the reduce function (a single process)? i.e. a database running as a single node with 8 shards would have at most 8 reduce functions running? So theoretically speaking increasing shards would decrease the time taking to build MapReduce indexes?


[1:11]
I now understand what I see 8 instances of the couchjs process in Windows task manager


jan [1:11 PM]
the reduce, especially built-in reduce is not likely to be factor in view build times.


[1:11]
oh


    [1:11]
windows


    [1:11]
windows has extra slowness added to it


zach [1:12 PM]
why is that?


jan [1:13 PM]
Windows isn’t very good at unix-style inter process communication


zach [1:14 PM]
so there is an overhead every time a map process is spawned and returned...


jan [1:15 PM]
it’s less the spwning, and more the i/o